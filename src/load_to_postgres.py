#!/usr/bin/env python3
"""
load_to_postgres.py - Loads processed Parquet files into PostgreSQL.

Reads database configuration from .env file.
Uses pandas and psycopg2 to directly load Parquet files into PostgreSQL tables.
Checks for existing data and prompts the user before dropping/reloading.

Prerequisites:
  - pip install psycopg2-binary python-dotenv pandas pyarrow
  - PostgreSQL server running with credentials in .env
  - PostgreSQL user with CREATE/DROP/INSERT privileges
  - generated_config.py must exist (created by generate_config.py)
  - Parquet files generated by data_cleaner.py
"""

import os
import sys
import logging
import argparse
from pathlib import Path
from typing import List, Dict, Any, Optional
import psycopg2
from psycopg2 import sql
from io import StringIO
import pandas as pd
from dotenv import load_dotenv

# --- Configuration ---
# Default path for Parquet files, matching the default output of data_cleaner.py
default_output_base = os.path.join(os.getenv('ProgramData', 'C:/ProgramData'), 'ETL_Pipeline_Output')
DEFAULT_PARQUET_DIR = Path(default_output_base)
DEFAULT_ENV_PATH = Path(".env")

# Configure logging
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    handlers=[logging.StreamHandler(sys.stdout)])
logger = logging.getLogger(__name__)

# --- Database Functions ---

def get_db_connection(env_path: Path):
    """Establishes a connection to the PostgreSQL database using .env settings."""
    if not env_path.exists():
        logger.error(f".env file not found at: {env_path.resolve()}")
        sys.exit(1)

    load_dotenv(dotenv_path=env_path)

    required_vars = ['DB_HOST', 'DB_PORT', 'DB_NAME', 'DB_USER', 'DB_PASSWORD']
    missing_vars = [var for var in required_vars if not os.getenv(var)]
    if missing_vars:
        logger.error(f"Missing required environment variables: {', '.join(missing_vars)}")
        sys.exit(1)

    conn = None
    try:
        conn = psycopg2.connect(
            host=os.getenv('DB_HOST'),
            port=os.getenv('DB_PORT'),
            dbname=os.getenv('DB_NAME'),
            user=os.getenv('DB_USER'),
            password=os.getenv('DB_PASSWORD')
        )
        logger.info(f"Successfully connected to database '{os.getenv('DB_NAME')}' on {os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}")
        return conn
    except psycopg2.OperationalError as e:
        logger.error(f"Database connection failed: {e}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"An unexpected error occurred during connection: {e}")
        if conn:
            conn.close()
        sys.exit(1)

def check_existing_data(conn, target_tables: List[str]) -> List[str]:
    """Checks if target tables exist and contain any data."""
    tables_with_data = []
    with conn.cursor() as cur:
        for table_name in target_tables:
            try:
                # Check if table exists first
                cur.execute("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables
                        WHERE table_schema = 'public' AND table_name = %s
                    );
                """, (table_name,))
                table_exists = cur.fetchone()[0]

                if table_exists:
                    # Check if table has data
                    query = sql.SQL("SELECT EXISTS (SELECT 1 FROM {table} LIMIT 1);").format(
                        table=sql.Identifier(table_name)
                    )
                    cur.execute(query)
                    has_data = cur.fetchone()[0]
                    if has_data:
                        logger.warning(f"Table '{table_name}' already exists and contains data.")
                        tables_with_data.append(table_name)
                    else:
                         logger.info(f"Table '{table_name}' exists but is empty.")
                else:
                    logger.info(f"Table '{table_name}' does not exist yet.")

            except psycopg2.Error as e:
                logger.error(f"Error checking table '{table_name}': {e}")
                # Continue checking other tables
    return tables_with_data

def terminate_existing_connections(conn, dbname):
    """Terminates all existing database connections except our own."""
    logger.info(f"Attempting to terminate existing connections to database '{dbname}'...")
    try:
        with conn.cursor() as cur:
            # Set our connection to auto-commit mode to allow this admin command
            conn.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)
            
            # Execute the terminate command
            cur.execute("""
                SELECT pg_terminate_backend(pid) 
                FROM pg_stat_activity 
                WHERE datname = %s 
                AND pid <> pg_backend_pid()
            """, (dbname,))
            
            # Reset isolation level
            conn.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_READ_COMMITTED)
            
            logger.info("Successfully terminated existing database connections")
            return True
    except psycopg2.Error as e:
        logger.error(f"Error while terminating connections: {e}")
        return False

def drop_table(conn, table_name: str) -> bool:
    """Drops a table if it exists."""
    drop_query = sql.SQL("DROP TABLE IF EXISTS {} CASCADE;").format(sql.Identifier(table_name))
    
    with conn.cursor() as cur:
        try:
            logger.info(f"Dropping table {table_name}...")
            cur.execute(drop_query)
            conn.commit()
            logger.info(f"Table {table_name} dropped successfully")
            return True
        except psycopg2.Error as e:
            conn.rollback()
            logger.error(f"Error dropping table {table_name}: {e}")
            return False

def create_table(conn, table_name: str, config: Dict):
    """Creates a target table with the specified columns."""
    column_defs = []
    
    # Add standard columns
    column_defs.append(f"id SERIAL PRIMARY KEY")
    column_defs.append(f"data_source VARCHAR(50) NOT NULL")
    column_defs.append(f"processed_at TIMESTAMP NOT NULL")
    column_defs.append(f"is_potential_duplicate BOOLEAN DEFAULT FALSE")
    
    # Add columns from configuration
    for mapped_name, col_spec in config["columns"].items():
        sql_type = col_spec["sql_type"]
        
        # Special handling for monetary columns to ensure precision
        if mapped_name == "transaction_amount" or mapped_name.endswith("_amount") or mapped_name.endswith("_total"):
            # Use NUMERIC with precision for monetary values
            column_defs.append(f'"{mapped_name}" NUMERIC(18,2)')
        else:
            column_defs.append(f'"{mapped_name}" {sql_type}')
    
    # Create the table
    create_query = sql.SQL("CREATE TABLE {} (\n  {}\n);").format(
        sql.Identifier(table_name),
        sql.SQL(",\n  ").join([sql.SQL(col_def) for col_def in column_defs])
    )
    
    with conn.cursor() as cur:
        try:
            logger.info(f"Creating table {table_name}...")
            cur.execute(create_query)
            conn.commit()
            logger.info(f"Table {table_name} created successfully")
            return True
        except psycopg2.Error as e:
            conn.rollback()
            logger.error(f"Error creating table {table_name}: {e}")
            return False

def load_parquet_to_postgres(conn, parquet_file: str, table_name: str, config: Dict) -> bool:
    """
    Load data directly from a Parquet file to PostgreSQL using pandas and psycopg2.
    
    Args:
        conn: PostgreSQL connection
        parquet_file: Path to Parquet file
        table_name: Target table name
        config: Source configuration from generated_config.py
        
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        logger.info(f"Loading data from {parquet_file} to table {table_name}...")
        
        # Get columns expected in the target table (excluding id which is auto-generated)
        # This includes metadata columns and data columns
        metadata_cols = ["data_source", "processed_at", "is_potential_duplicate"]
        data_cols = list(config["columns"].keys())
        target_cols = metadata_cols + data_cols
        
        # Read the Parquet file
        df = pd.read_parquet(parquet_file)
        input_row_count = len(df)
        logger.info(f"Read {input_row_count} rows from {parquet_file}")
        
        # Log basic stats about key columns for verification
        if "transaction_amount" in df.columns:
            null_count = df["transaction_amount"].isna().sum()
            logger.info(f"transaction_amount null count: {null_count}/{len(df)} ({null_count/len(df)*100:.1f}%)")
            non_null = df["transaction_amount"].dropna()
            if not non_null.empty:
                logger.info(f"Sample transaction_amount values: {non_null.head(5).tolist()}")
        
        # Check for missing columns and add them with nulls if needed
        for col in target_cols:
            if col not in df.columns:
                logger.warning(f"Column '{col}' missing in Parquet file, adding with NULL values")
                df[col] = None
        
        # Keep only the columns we need
        cols_to_keep = [col for col in target_cols if col in df.columns]
        df = df[cols_to_keep]
        
        # Find time columns and properly handle NULLs
        time_columns = []
        for col_name, col_spec in config["columns"].items():
            if col_spec["sql_type"].lower().startswith("time"):
                time_columns.append(col_name)
        
        # Convert NaN/NaT values to None for time columns
        for col in time_columns:
            if col in df.columns:
                df[col] = df[col].where(df[col].notna(), None)
                # Handle "<NA>" string values
                if df[col].dtype == 'object' or pd.api.types.is_string_dtype(df[col].dtype):
                    df[col] = df[col].apply(lambda x: None if isinstance(x, str) and (x == "<NA>" or x.lower() == "none") else x)
        
        # Find date columns and properly handle "None" string values
        date_columns = []
        for col_name, col_spec in config["columns"].items():
            if col_spec["sql_type"].lower().startswith("date"):
                date_columns.append(col_name)
        
        # Convert NaN/NaT and "None" string values to None for date columns
        for col in date_columns:
            if col in df.columns:
                # Handle NaT values
                df[col] = df[col].where(df[col].notna(), None)
                # Handle "None" and "<NA>" string values
                if df[col].dtype == 'object' or pd.api.types.is_string_dtype(df[col].dtype):
                    df[col] = df[col].apply(lambda x: None if isinstance(x, str) and (x.lower() == "none" or x == "<NA>") else x)
        
        # Find monetary columns for special handling
        monetary_columns = []
        for col_name in df.columns:
            if col_name == "transaction_amount" or col_name.endswith("_amount") or col_name.endswith("_total"):
                monetary_columns.append(col_name)
        
        # Ensure proper handling of monetary values
        for col in monetary_columns:
            if col in df.columns:
                # Keep nulls as nulls, but ensure numeric formatting
                non_null_mask = df[col].notna()
                if non_null_mask.any():
                    # Ensure proper numeric format without truncation
                    df.loc[non_null_mask, col] = df.loc[non_null_mask, col].astype(float)
        
        # Find and handle integer columns to ensure proper INTEGER type in PostgreSQL
        integer_columns = []
        for col_name, col_spec in config["columns"].items():
            if col_spec["sql_type"].lower() == "integer" and col_name in df.columns:
                integer_columns.append(col_name)
        
        # Convert float columns that should be integers to actual integers
        for col in integer_columns:
            if col in df.columns and df[col].dtype != 'int64':
                # Handle NULL values first
                null_mask = df[col].isna()
                
                # Convert non-null values to integers
                if (~null_mask).any():
                    # Use floordiv to remove decimal part and explicitly convert to Python int
                    temp_series = df[col].copy()
                    # First convert to floordiv(1) to remove decimal part
                    temp_series[~null_mask] = temp_series[~null_mask].floordiv(1)
                    
                    # Now explicitly convert to Python int objects to ensure proper CSV formatting
                    # This ensures no decimal points appear in CSV output
                    temp_series[~null_mask] = temp_series[~null_mask].astype('Int64')
                    
                    # Convert to strings without decimals for CSV export
                    temp_series[~null_mask] = temp_series[~null_mask].astype(str).str.replace('.0$', '', regex=True)
                    
                    df[col] = temp_series
                    
                    # Log the conversion
                    logger.info(f"Converted column '{col}' from {df[col].dtype} to integer")
        
        # Create column string for INSERT
        columns_str = ", ".join([f'"{col}"' for col in cols_to_keep])
        
        # Use the connection's cursor to perform the COPY operation
        success_count = 0
        error_count = 0
        chunk_size = 100000  # Process in chunks to avoid memory issues
        
        # Get the total number of chunks
        total_chunks = (len(df) + chunk_size - 1) // chunk_size
        
        for i in range(0, len(df), chunk_size):
            chunk = df.iloc[i:i+chunk_size]
            chunk_num = i // chunk_size + 1
            logger.info(f"Processing chunk {chunk_num}/{total_chunks} ({len(chunk)} rows)")
            
            # Create a buffer for the COPY command
            buffer = StringIO()
            
            # Write the chunk to CSV format in memory
            # Use tab as separator to avoid issues with commas in text fields
            # IMPORTANT: Do NOT convert NULL values to 'None' strings and ensure proper NULL representation
            # PostgreSQL uses \N to represent NULL in COPY operations
            chunk.to_csv(buffer, index=False, header=False, sep='\t', na_rep='\\N')
            buffer.seek(0)
            
            try:
                with conn.cursor() as cursor:
                    # Use COPY command which is much faster than individual INSERTs
                    copy_sql = f'COPY "{table_name}" ({columns_str}) FROM STDIN WITH (FORMAT CSV, DELIMITER E\'\\t\', NULL \'\\N\')'
                    cursor.copy_expert(copy_sql, buffer)
                
                conn.commit()
                success_count += len(chunk)
                logger.info(f"Chunk {chunk_num}/{total_chunks} loaded successfully. Total rows loaded: {success_count}")
                
            except Exception as e:
                conn.rollback()
                error_count += 1
                logger.error(f"Error loading chunk {chunk_num}/{total_chunks}: {e}")
                
                # If first chunk fails, try to diagnose the issue
                if chunk_num == 1:
                    logger.error("First chunk failed. Diagnosing potential data issues...")
                    # Sample problematic rows for analysis
                    sample_rows = chunk.head(5).to_dict('records')
                    logger.error(f"Sample rows: {sample_rows}")
                
                if error_count > 3:  # Allow a few retries before failing completely
                    logger.error("Too many errors, aborting.")
                    return False
        
        # Verify the row count in the database matches the input count
        with conn.cursor() as cursor:
            cursor.execute(sql.SQL("SELECT COUNT(*) FROM {};").format(sql.Identifier(table_name)))
            db_row_count = cursor.fetchone()[0]
            
            if db_row_count != input_row_count:
                logger.error(f"ROW COUNT MISMATCH: Expected {input_row_count} rows but found {db_row_count} rows in table {table_name}")
                return False
            else:
                logger.info(f"Row count verification successful: {db_row_count} rows in database matches input file row count")
        
        if success_count > 0:
            logger.info(f"Successfully loaded {success_count} rows to table {table_name}")
            return True
        else:
            logger.error(f"No rows were loaded to table {table_name}")
            return False
        
    except Exception as e:
        logger.error(f"Error loading Parquet file {parquet_file}: {e}")
        return False

# --- Main Execution ---

def main():
    parser = argparse.ArgumentParser(description="Load processed Parquet data into PostgreSQL.")
    parser.add_argument(
        "--parquet-dir",
        type=Path,
        default=DEFAULT_PARQUET_DIR,
        help=f"Path to the directory containing Parquet files (default: {DEFAULT_PARQUET_DIR})"
    )
    parser.add_argument(
        "--env-file",
        type=Path,
        default=DEFAULT_ENV_PATH,
        help=f"Path to the .env file (default: {DEFAULT_ENV_PATH})"
    )
    parser.add_argument(
        '-y', '--yes',
        action='store_true',
        help="Automatically confirm dropping existing data without prompting."
    )

    args = parser.parse_args()
    
    # Log all arguments for debugging
    logger.info(f"Running with args: parquet-dir={args.parquet_dir}, env-file={args.env_file}, yes={args.yes}")

    # --- Import generated config ---
    try:
        from generated_config import CONFIG
        logger.info("Successfully imported configuration from generated_config.py")
    except ImportError:
        logger.critical("Could not find generated_config.py. Run generate_config.py first.")
        sys.exit(1)
    except Exception as e:
        logger.critical(f"Error importing generated_config.py: {e}")
        sys.exit(1)

    target_tables = [cfg["target_table"] for cfg in CONFIG.values()]
    if not target_tables:
        logger.error("No target tables found in generated_config.py. Cannot proceed.")
        sys.exit(1)

    logger.info(f"Target tables identified: {', '.join(target_tables)}")
    
    # Prepare dictionary of Parquet files
    parquet_files = {}
    for source, source_config in CONFIG.items():
        target_table = source_config["target_table"]
        parquet_files[target_table] = []
        
        # Find corresponding Parquet files
        for file_path in source_config["files"]:
            file_name = Path(file_path).stem
            parquet_file = args.parquet_dir / f"{source}_{file_name}.parquet"
            if parquet_file.exists():
                parquet_files[target_table].append(str(parquet_file))
            else:
                logger.warning(f"Parquet file not found: {parquet_file}")
        
        if not parquet_files[target_table]:
            logger.warning(f"No Parquet files found for source {source}")

    # Connect to the database
    conn = None
    try:
        # --- Connect to database ---
        conn = get_db_connection(args.env_file)
        
        # --- Check Existing Data ---
        tables_with_data = check_existing_data(conn, target_tables)

        # --- User Confirmation ---
        proceed = False
        if tables_with_data:
            logger.info(f"Found {len(tables_with_data)} tables with data: {', '.join(tables_with_data)}")
            
            if args.yes:
                logger.info("--yes flag provided. Proceeding with dropping and reloading data without confirmation.")
                proceed = True
            else:
                logger.info("No --yes flag. Asking for confirmation...")
                print("-" * 50)
                print("WARNING: The following target tables already contain data:")
                for table in tables_with_data:
                    print(f"  - {table}")
                print("This operation will DROP these tables before loading data.")
                print("-" * 50)
                confirm = input("Do you want to proceed with dropping and reloading? (yes/no): ").strip().lower()
                if confirm == 'yes':
                    logger.info("User confirmed to proceed with dropping and reloading.")
                    proceed = True
                else:
                    logger.info("User declined to proceed. Operation aborted.")
                    proceed = False
        else:
            logger.info("No existing data found in target tables. Proceeding with load.")
            proceed = True # No need to prompt if tables are empty or don't exist

        # --- Execute Data Load ---
        if proceed:
            logger.info("Beginning data load process...")
            
            # First, terminate any existing connections to the database
            # This is critical to avoid lock issues when dropping tables
            logger.info("Terminating existing connections...")
            if terminate_existing_connections(conn, os.getenv('DB_NAME')):
                logger.info("Successfully terminated existing connections.")
            else:
                logger.warning("Failed to terminate some connections. Proceeding anyway.")
            
            # Process each source
            for source, source_config in CONFIG.items():
                target_table = source_config["target_table"]
                logger.info(f"Processing source: {source}, target table: {target_table}")
                
                # Drop existing table
                logger.info(f"Attempting to drop table {target_table} if it exists...")
                if not drop_table(conn, target_table):
                    logger.error(f"Failed to drop table {target_table}. Skipping.")
                    continue
                
                # Create new table
                logger.info(f"Creating new table {target_table}...")
                if not create_table(conn, target_table, source_config):
                    logger.error(f"Failed to create table {target_table}. Skipping.")
                    continue
                
                # Load data from Parquet files
                table_files = parquet_files[target_table]
                if not table_files:
                    logger.warning(f"No Parquet files to load for table {target_table}")
                    continue
                
                logger.info(f"Found {len(table_files)} Parquet files to load for {target_table}")
                for parquet_file in table_files:
                    logger.info(f"Loading file: {parquet_file}")
                    if not load_parquet_to_postgres(conn, parquet_file, target_table, source_config):
                        logger.error(f"Failed to load data from {parquet_file} to {target_table}")
                    else:
                        logger.info(f"Successfully loaded data from {parquet_file} to {target_table}")
            
            logger.info("Data loading completed.")
        else:
             logger.info("Load operation cancelled.")

    finally:
        # --- Close Connection ---
        if conn:
            conn.close()
            logger.info("Database connection closed.")


if __name__ == "__main__":
    main() 